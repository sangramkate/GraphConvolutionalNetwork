#include <stdlib.h>
#include <assert.h>
#include <iostream>
#include <random>

#include "linear_layer.hh"
#include "nn_exception.hh"

__global__ void linearLayerForward( float* W, float* A, float* Z, float* b,
                                                                           int W_x_dim, int W_y_dim,
                                                                           int A_x_dim, int A_y_dim){
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
  
    int Z_x_dim = A_x_dim;
    int Z_y_dim = W_x_dim;
  
    float Z_value = 0;
  
    if( row < Z_x_dim && col < Z_y_dim){
       for(int i=0; i< W_y_dim; i=i+1){
           Z_value += W[i + W_y_dim * col] * A[i + A_y_dim * row]; 
       }
       Z[row * Z_y_dim + col] = Z_value + b[col];
    }
}

__global__ void linearLayerBackprop( float* W, float* dZ, float*dA,
                                                                    int W_x_dim, int W_y_dim,
                                                                    int dZ_x_dim, int dZ_y_dim){
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
  
    int dA_x_dim = dZ_x_dim;
    int dA_y_dim = W_y_dim;
  
    float dA_value = 0.0f;
  	if (row < dA_x_dim && col < dA_y_dim) {
		    for (int i = 0; i < W_x_dim; i++) {
			      dA_value += W[i * W_y_dim + col] * dZ[ i + dZ_y_dim * row];
		    }
		    dA[row * dA_y_dim + col] = dA_value;
	  }
}

__global__ void linearLayerUpdateWeights(  float* dZ, float* A, float* W,
										   int dZ_x_dim, int dZ_y_dim,
										   int A_x_dim, int A_y_dim,
										   float learning_rate) {

	int col = blockIdx.x * blockDim.x + threadIdx.x;
	int row = blockIdx.y * blockDim.y + threadIdx.y;

	// A is treated as transposed
	int W_x_dim = dZ_y_dim;
	int W_y_dim = A_y_dim;

	float dW_value = 0.0f;

	if (row < W_y_dim && col < W_x_dim) {
		for (int i = 0; i < dZ_x_dim; i++) {
			dW_value += dZ[i * dZ_y_dim + col ] * A[row + A_y_dim * i];
		}
		W[col * W_y_dim + row] = W[col * W_y_dim + row] - learning_rate * (dW_value / A_y_dim);
	}
}

__global__ void linearLayerUpdateBias(  float* dZ, float* b,
										int dZ_x_dim, int dZ_y_dim,
										int b_x_dim,
										float learning_rate) {
	int index = blockIdx.y * blockDim.y + threadIdx.y;

	if (index < dZ_x_dim * dZ_y_dim) {
		int dZ_x = index % dZ_y_dim;
		int dZ_y = index / dZ_y_dim;
		atomicAdd(&b[dZ_y], - learning_rate * (dZ[dZ_y * dZ_y_dim + dZ_x] / dZ_y_dim));
	}
}

LinearLayer::LinearLayer(std::string name, Shape W_shape):
    W(W_shape),b(W_shape.y,1)
{
    this->name = name;
    std::cout << "updated layer name\n";
    b.allocateCudaMemory();
    std::cout << "b allocated\n";
    W.allocateMemory();
    std::cout << "w allocated\n";
    initializeBiasWithZeros();
    std::cout << "bias initialized\n";
    initializeWeightsRandomly();
    std::cout << "weights initialized\n";
}

LinearLayer::~LinearLayer()
{ };

void LinearLayer::initializeWeightsRandomly(){
    std::default_random_engine generator;
    std::normal_distribution<float> normal_distribution(0.0, 1.0);
    std::cout << "W.shape.x:" << W.shape.x <<"\n";	
    std::cout << "W.shape.y:" << W.shape.y <<"\n";	
    for(int x = 0; x < W.shape.x; x++){
	for(int y = 0 ; y < W.shape.y; y++){
	     W[x * W.shape.y + y] = normal_distribution(generator) * weights_init_threshold;	
	}
    }
    std::cout << "copying data from host to device\n";
    W.copyHostToDevice();
    free(W.data_host);
}

void LinearLayer::initializeBiasWithZeros() {
	//for (int x = 0; x < b.shape.x; x++) {
	//	b[x] = 0;
	//}
	//b.copyHostToDevice();
        cudaMemset(b.data_device, 0, b.shape.x * b.shape.y* sizeof(float));
}

Matrix& LinearLayer::forward(Matrix& A, bool training, bool freeMatrix){
    assert(W.shape.y = A.shape.y);
    this->A = A;
    Shape Z_shape(A.shape.x,W.shape.x);
    Z.allocateCuda(Z_shape);
    computeAndStoreLayerOutput(A);
    std::cout << "Linear Layer forward\n";
    NNException::throwIfDeviceErrorOccurred("Cannot perform Linear Layer forward propagation");
    
    if(freeMatrix)
        A.freeMem();
    return Z;
	
}
void LinearLayer::computeAndStoreLayerOutput(Matrix& A) {
dim3 block_size(32,32);
dim3 num_of_blocks(((Z.shape.x + block_size.x - 1) / block_size.x),((Z.shape.y + block_size.y - 1) / block_size.y) );
linearLayerForward<<<num_of_blocks, block_size>>>( W.data_device,
				                   A.data_device,
						   Z.data_device,
						   b.data_device,
						   W.shape.x, W.shape.y,
						   A.shape.x, A.shape.y);
}

Matrix& LinearLayer::backprop(Matrix& dZ, float learning_rate) {
	dA.allocateCuda(A.shape);

        std::cout << "Linear Layer backward\n";
	computeAndStoreBackpropError(dZ);
	NNException::throwIfDeviceErrorOccurred("Cannot perform back propagation.");

	updateBias(dZ, learning_rate);
	NNException::throwIfDeviceErrorOccurred("Cannot perform bias update.");

	updateWeights(dZ, learning_rate);
	NNException::throwIfDeviceErrorOccurred("Cannot perform weights update.");

        dZ.freeMem();
	return dA;
}

void LinearLayer::computeAndStoreBackpropError(Matrix& dZ) {
	dim3 block_size(32, 32);
	dim3 num_of_blocks((A.shape.x + block_size.x - 1) / block_size.x,(A.shape.y + block_size.y - 1) / block_size.y);
	linearLayerBackprop<<<num_of_blocks, block_size >>> ( W.data_device,
							     dZ.data_device,
							     dA.data_device,
							     W.shape.x, W.shape.y,
							     dZ.shape.x, dZ.shape.y);
}

void LinearLayer::updateWeights(Matrix& dZ, float learning_rate) {
	dim3 block_size(32, 32);
	dim3 num_of_blocks((W.shape.x + block_size.x - 1) / block_size.x,(W.shape.y + block_size.y - 1) / block_size.y);
	linearLayerUpdateWeights<<<num_of_blocks, block_size>>>(dZ.data_device,
								A.data_device,
								W.data_device,
								dZ.shape.x, dZ.shape.y,
								A.shape.x, A.shape.y,
								learning_rate);
}

void LinearLayer::updateBias(Matrix& dZ, float learning_rate) {
	dim3 block_size(256);
	dim3 num_of_blocks( (dZ.shape.y * dZ.shape.x + block_size.x - 1) / block_size.x);
	linearLayerUpdateBias<<<num_of_blocks, block_size>>>(dZ.data_device,
							     b.data_device,
							     dZ.shape.x, dZ.shape.y,
							     b.shape.x, learning_rate);
}

int LinearLayer::getXdim() const {
	return W.shape.x;
}

int LinearLayer::getYdim() const {
	return W.shape.y;
}

Matrix LinearLayer::getWeightsMatrix() const {
	return W;
}

Matrix LinearLayer::getBiasVector() const {
	return b;
}
	    
